{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkSQL Script\n",
    "Author: Evan Bariquit\n",
    "\n",
    "1. Time to count the occurences of each element in the file\n",
    "2. Time to alphabetize the elements\n",
    "2. Time to total the amounts for each transCat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.functions import trim\n",
    "from pyspark.sql.functions import sum\n",
    "from pyspark.sql.types import IntegerType\n",
    "import timeit\n",
    "\n",
    "\n",
    "# Create Spark Session.\n",
    "sparkContext = SparkContext()\n",
    "spark = SparkSession(sparkContext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset.\n",
    "# 'header = True' b/c our file has column headers\n",
    "upload_startTime = timeit.default_timer()\n",
    "datalog = spark.read.load(\"Data/datalog2.csv\", format=\"csv\", header=True, inferSchema=True)\n",
    "upload_stopTime = timeit.default_timer()\n",
    "\n",
    "# Remove leading/trailing whitespaces\n",
    "for col_name in datalog.columns:\n",
    "    datalog = datalog.withColumn(col_name, trim(col(col_name)))\n",
    "\n",
    "# Spark interprets the schema of these columns as strings; convert to integers.\n",
    "datalog = datalog.withColumn(\"distance\", col(\"distance\").cast(IntegerType()))\n",
    "datalog = datalog.withColumn(\"amount\", col(\"amount\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-----+--------+-------------+-------------+------+\n",
      "| vendor|       city|state|distance|     transCat|    transType|amount|\n",
      "+-------+-----------+-----+--------+-------------+-------------+------+\n",
      "|    QFC|   Kirkland|   WA|       8|wire transfer|    gift card|   160|\n",
      "|Safeway|    Seattle|   WA|      20|    withdrawl|          ATM|   118|\n",
      "|Safeway|Woodinville|   WA|       0|      grocery|personal care|     6|\n",
      "|    QFC|   Kirkland|   WA|       8|  electronics|           TV|  4647|\n",
      "|Safeway|Woodinville|   WA|       0|   restaurant|        steak|   305|\n",
      "|Safeway|Woodinville|   WA|       0|   restaurant|      dim sum|   156|\n",
      "| Amazon|     Online|   NA|       0|     clothing|        shoes|   154|\n",
      "| Amazon|     Online|   NA|       0|  electronics|       stereo|  2520|\n",
      "| Amazon|     Online|   NA|       0|   restaurant|    fast food|    88|\n",
      "|Safeway|    Seattle|   WA|      20|      grocery|         food|  1766|\n",
      "|    QFC|   Kirkland|   WA|       8|      grocery|personal care|   167|\n",
      "| Amazon|     Online|   NA|       0|     clothing|        jeans|   275|\n",
      "|    QFC|   Kirkland|   WA|       8|      grocery|         food|    31|\n",
      "|Safeway|    Seattle|   WA|      20|    withdrawl|          ATM|   173|\n",
      "|Safeway|Woodinville|   WA|       0|entertainment|       dining|   780|\n",
      "|Safeway|    Seattle|   WA|      20|     clothing|        shirt|    35|\n",
      "|    QFC|   Kirkland|   WA|       8|    withdrawl|          ATM|   192|\n",
      "|Safeway|    Seattle|   WA|      20|entertainment|        movie|   120|\n",
      "|Safeway|    Seattle|   WA|      20|      grocery|personal care|   183|\n",
      "| Amazon|     Online|   NA|       0|  electronics|           TV|  4455|\n",
      "+-------+-----------+-----+--------+-------------+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Time to load the dataset into a dataframe: 25.025954400000003\n"
     ]
    }
   ],
   "source": [
    "# For viewing a sample of the data\n",
    "datalog.show()\n",
    "print(\"Time to load the dataset into a dataframe:\", upload_stopTime - upload_startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting frequencies of each element\n",
    "\n",
    "Implementation:\n",
    "    1. Number of occurences will be stored in the 'counts' spark dataframe.\n",
    "    2. Parse each column of the data file.\n",
    "        a. get occurences of elements within column.\n",
    "        b. store occurences in 'counts'\n",
    "        NOTE: this creates duplicates!\n",
    "        eg: 'QFC' appears in 'vendor' and 'transType' columns.\n",
    "            We will have two counts for QFC (one for its occurences \n",
    "            in vendor, the other for occurences in transType).\n",
    "    3. Condense duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to count frequencies: 0.05750580000000127\n"
     ]
    }
   ],
   "source": [
    "# Where we will store the counts per element\n",
    "counts = spark.createDataFrame([('test',0)],['element','count'])\n",
    "\n",
    "# Begin.\n",
    "counting_startTime = timeit.default_timer()\n",
    "\n",
    "for column in datalog.columns:\n",
    "    subset = datalog.groupBy(column).count().withColumnRenamed(column, 'element')\n",
    "    # NOTE:\n",
    "        # (Distance/Amount)\n",
    "        # Do we want to count occurences for these columns? \n",
    "        # They aren't counted in Stephen's implementation, \n",
    "        # so skipping them here for consistency.\n",
    "    if column != 'distance' and column != 'amount':\n",
    "        counts = counts.unionByName(subset)\n",
    "\n",
    "# Condenses duplicates.\n",
    "counts = counts.groupBy('element').agg(sum('count').alias('count'))\n",
    "\n",
    "# Finished.\n",
    "counting_stopTime = timeit.default_timer()\n",
    "#counts.show()\n",
    "print(\"Time to count frequencies:\", counting_stopTime - counting_startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting the above dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to sort: 0.005358300000011695\n"
     ]
    }
   ],
   "source": [
    "# Begin.\n",
    "sorting_startTime = timeit.default_timer()\n",
    "\n",
    "counts = counts.sort(\"element\", ascending=True)\n",
    "\n",
    "# Finished.\n",
    "sorting_stopTime = timeit.default_timer()\n",
    "#counts.show()\n",
    "print(\"Time to sort:\", sorting_stopTime - sorting_startTime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total the amounts per transCat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to total the amounts spent by transCat: 0.008206600000050912\n"
     ]
    }
   ],
   "source": [
    "# Begin.\n",
    "amounts_startTime = timeit.default_timer()\n",
    "\n",
    "totalByCat = datalog.groupBy('transCat').agg(sum('amount').alias('total'))\n",
    "\n",
    "# Finished.\n",
    "amounts_stopTime = timeit.default_timer()\n",
    "#totalByCat.show()\n",
    "print(\"Time to total the amounts spent by transCat:\", amounts_stopTime - amounts_startTime)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
