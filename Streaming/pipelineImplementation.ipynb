{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.streaming import StreamingContext\n",
    "import pyspark.sql.types as SQLTypes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row, Column\n",
    "import sys\n",
    "\n",
    "# Create Spark Session.\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+--------------------+\n",
      "| id|label|               tweet|\n",
      "+---+-----+--------------------+\n",
      "|  1|    0| @user when a fat...|\n",
      "|  2|    0|@user @user thank...|\n",
      "|  3|    0|  bihday your maj...|\n",
      "|  4|    0|#model   i love u...|\n",
      "|  5|    0| factsguide: soci...|\n",
      "+---+-----+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the schema of our dataset\n",
    "my_schema = SQLTypes.StructType([\n",
    "    SQLTypes.StructField(name='id', dataType=SQLTypes.IntegerType(), nullable=True),\n",
    "    SQLTypes.StructField(name='label', dataType=SQLTypes.IntegerType(), nullable=True),\n",
    "    SQLTypes.StructField(name='tweet', dataType=SQLTypes.StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Load the csv\n",
    "my_data = spark.read.csv('Twitter_HateSpeech.csv', schema=my_schema, header=True)\n",
    "\n",
    "# For visualizing\n",
    "my_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our ML pipeline, which we will pass streamed tweets through to generate a prediction\n",
    "\n",
    "# Stage 1: Tokenize the tweet\n",
    "stage1 = RegexTokenizer(inputCol = 'tweet', outputCol = 'tokens', pattern = '\\\\W')\n",
    "\n",
    "# Stage 2: Remove stop words\n",
    "stage2 = StopWordsRemover(inputCol = 'tokens', outputCol = 'filtered_words')\n",
    "\n",
    "# Stage 3: Create a word vector (size 100 will do, since tweets can only contain 140 characters max)\n",
    "stage3 = Word2Vec(inputCol = 'filtered_words', outputCol = 'vector', vectorSize = 100)\n",
    "\n",
    "# Stage 4: Pass the data into a logistic regression model\n",
    "model = LogisticRegression(featuresCol = 'vector', labelCol = 'label')\n",
    "\n",
    "\n",
    "\n",
    "# Assemble the pipleline\n",
    "pipeline = Pipeline(stages = [stage1, stage2, stage3, model])\n",
    "\n",
    "pipelineFit = pipeline.fit(my_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(tweet_text):\n",
    "    try:\n",
    "        # remove blank tweets\n",
    "        tweet_text = tweet_text.filter(lambda x: len(x) > 0)\n",
    "        \n",
    "        # Create the dataframe with each row being a tweet text\n",
    "        rowRdd = tweet_text.map(lambda w: Row(tweet=w))\n",
    "        wordsDataFrame = spark.createDataFrame(rowRdd)\n",
    "        \n",
    "        # get the prediction for each row\n",
    "        df = pipelineFit.transform(wordsDataFrame).select('tweet', 'prediction').show()\n",
    "        \n",
    "        send_df_to_dashboard(df)\n",
    "        \n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO\n",
    "1. alter send_df_to_dashboard() to handle the new dataframe schema coming in. (See ouput of 2nd print() statement below for schema details).\n",
    "2. alter main() - change 'words.foreachRDD(process)' to 'words.foreachRDD(get_prediction)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9389920424403183\n",
      "DataFrame[id: int, label: int, tweet: string, tokens: array<string>, filtered_words: array<string>, vector: vector, rawPrediction: vector, probability: vector, prediction: double]\n"
     ]
    }
   ],
   "source": [
    "# this section not needed, used for testing only.\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "test_data = spark.read.csv('Twitter_HateSpeech_test.csv', schema=my_schema, header=False)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", \n",
    "                                               predictionCol=\"prediction\", \n",
    "                                               metricName=\"accuracy\")\n",
    "\n",
    "test_predictions = pipelineFit.transform(test_data)\n",
    "print(\"Accuracy:\", evaluator.evaluate(test_predictions))\n",
    "print(test_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
