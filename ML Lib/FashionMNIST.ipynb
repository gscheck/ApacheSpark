{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <u>Image classification using Spark's MLlib and the MNIST dataset</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UI Web URL: http://DESKTOP-JI84C36:4040\n"
     ]
    }
   ],
   "source": [
    "# Import libraries.\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from time import time\n",
    "\n",
    "# Create Spark Session.\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "print(\"UI Web URL:\",sc.uiWebUrl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PySpark DF from CSV file.\n",
    "df_training = (spark\n",
    "               .read\n",
    "               .options(header = True, inferSchema = True) # 'header = True' b/c our file has column headers\n",
    "               .csv(\"NormalizedDataset/mnist_training.csv\")) \n",
    "\n",
    "# Removes the indexing column. Delete this if there is no index column in your csv.\n",
    "df_training = df_training.drop(df_training._c0) \n",
    "\n",
    "# Extract column names. Remove 'label' to get names of feature columns only. \n",
    "feature_columns = df_training.schema.names\n",
    "feature_columns.pop()\n",
    "\n",
    "# Vectorize the feature columns for each instance. \n",
    "vectorizer = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "\n",
    "# Map labels to their features vector. \n",
    "training = (vectorizer\n",
    "            .transform(df_training)\n",
    "            .select(\"label\", \"features\") # maps 'label' values from df_training to 'features' from vectorizer\n",
    "            .toDF(\"label\", \"features\") # assigns the column names in our new df\n",
    "            .cache())\n",
    "\n",
    "# For visualizing\n",
    "#training.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare the Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PySpark DF from CSV file.\n",
    "df_testing = (spark\n",
    "              .read\n",
    "              .options(header = True, inferSchema = True)\n",
    "              .csv(\"NormalizedDataset/mnist_testing.csv\"))\n",
    "\n",
    "# Remove indexing column.\n",
    "df_testing = df_testing.drop(df_testing._c0)\n",
    "\n",
    "# Map labels to their features vector. \n",
    "testing = (vectorizer\n",
    "           .transform(df_testing)\n",
    "           .select(\"label\", \"features\")\n",
    "           .toDF(\"label\", \"features\")\n",
    "           .cache())\n",
    "\n",
    "# For visualizing \n",
    "#testing.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8676\n",
      "Training Time: 2251\n"
     ]
    }
   ],
   "source": [
    "# Initialize classifier\n",
    "layers = [784, 100, 20, 10]\n",
    "# We have created a network for 784 input nodes (one for each pixel in our image),\n",
    "# 100 nodes in the first hidden layer, 20 in the second, and 10 in the ouput \n",
    "# (one for each digit the image represents: 0-9).\n",
    "perceptron = MultilayerPerceptronClassifier(maxIter=1000, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# Initialize evluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", \n",
    "                                               predictionCol=\"prediction\", \n",
    "                                               metricName=\"accuracy\")\n",
    "\n",
    "# Begin training\n",
    "start_time = time()\n",
    "perceptron_model = perceptron.fit(training)\n",
    "stop_time = time()\n",
    "\n",
    "# Generate predictions and evaluate performance.\n",
    "test_pred = perceptron_model.transform(testing)\n",
    "print(\"Accuracy:\", evaluator.evaluate(test_pred))\n",
    "print(\"Training Time: %d\" % (stop_time - start_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
